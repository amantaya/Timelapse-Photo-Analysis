## Horse-Cattle-Elk Grazing Interaction Study Rproj
## Step 8: Combine Chunks into Site-Year CSV File

## What this script does:
## Scans a directory for CSV files
## Combines CSV files for each site and year
## Writes out a single CSV file for each site and year "2020-BKS.csv"

## What this script requires:
## Must specify the location of the folder containing csv files
## Must specify where to write out CSV files

# set the working directory and environment variables
source("~/grazing-interaction/environment.R")

# load in the required packages
source("~/grazing-interaction/packages.R")

# load in the required functions
source("~/grazing-interaction/functions.R")

# set working directory to location of recombined csv files
# file.path() is system agnostic (i.e. works on Mac/PC/Linux)
path_to_recombined_csv_files <- file.path(currentwd, "data", "photo", "completed-xlsm", "csv", "recombined")

# list csv files that were recombined in the previous step
# we want to recombine these files into a single csv file for each year
csv_file_list <- list.files(path_to_recombined_csv_files, pattern = ".csv")

# convert to a tibble for pretty printing
csv_files_tibble <- tibble(csv_file_list)

# rename the first column to "path" to be more descriptive of what that column represents
names(csv_files_tibble)[names(csv_files_tibble) == "csv_file_list"] <- "path"

# create new columns using the name of each file as metadata
# we can use these new columns to filter and handle the data
csv_files_tibble_separated <- separate(csv_files_tibble, path,
                                   into = c("sitecode",
                                            "deploydate",
                                            "collectdate",
                                            "subjects",
                                            "all",
                                            "chunks"),
                                   sep = "_",
                                   remove = FALSE)

# create a year column that we can use to group by year
csv_files_tibble_separated <- csv_files_tibble_separated %>% dplyr::mutate(year = str_sub(collectdate, -4))

# unique sites
sitecodes_in_csv_files <- unique(csv_files_tibble_separated$sitecode)

# print the site codes in console
unique(csv_files_tibble_separated$sitecode)

# the data frame generated by list.files has the csv files for all sites
# but we want to combine data by site and write out a file for each site
# we can use dplyr to group by the site code
grouped_site_dataframes <- csv_files_tibble_separated %>% group_by(sitecode, year)

# manual fix for sites named with year first
# find the first 4 digits from the deploy date
year_first_pattern_match <- stringr::str_extract(grouped_site_dataframes$deploydate, pattern = "\\d{4}")

# then get the index for which files are named with the year first
# the caret ^ indicates the very start of the string in a 2
year_first_index <- stringr::str_which(year_first_pattern_match, pattern = "^2")

# extract the year using the index
subset_year_first_pattern_match <- year_first_pattern_match[year_first_index]

# assign the year back into the tibbles for the year first files
for (i in 1:length(subset_year_first_pattern_match)) {
  grouped_site_dataframes$year[year_first_index[i]] <- subset_year_first_pattern_match[i]
}

# and then use the group split to create a list
# each element corresponds to a site data frame
grouped_site_list <- dplyr::group_split(grouped_site_dataframes)

# this prints the first site in the list
grouped_site_list[[1]]

# get data used to group each data frame
group_dataframe <- group_data(grouped_site_dataframes)

# create a new "names" column that we can use to name elements in the list of data frames
group_names_df <- unite(group_dataframe, year, sitecode, col = name, sep = "_", remove = FALSE)

# set the names of the elements in the list by using the site codes
# this will help us access components by using their names (instead of index)
# e.g., grouped_site_list$BKS instead of grouped_site_list$1
names(grouped_site_list) <- group_names_df$name

# read in the data from each recombined csv file and bind rows together
for (i in 1:length(grouped_site_list)) {
  data <-
    file.path(path_to_recombined_csv_files, grouped_site_list[[i]]$path) %>%
    lapply(readr::read_csv) %>%
    dplyr::bind_rows()

  # construct a file name that uses the year from the data and the site code
  filename <- paste0(group_names_df$name[i], ".csv")

  # then convert the date back into a ISO string
  data$ImageDate <- strftime(data$DateTime, format = "%F")

  data$ImageTime <- strftime(data$DateTime, format = "%T")

  data$DateTime <- strftime(data$DateTime, usetz = TRUE)

  data$LastSavedOn <- strftime(data$LastSavedOn, usetz = TRUE)

  # write out the data as a csv file using the file name convention
  write_csv(data, file.path(currentwd, "data", "photo", "combined-by-site-year", "unprocessed", filename))
}

# get the current system time to notify when the script is completed
# note that this defaults to UTC (aka Greenwich Mean Time)
system_time <- Sys.time()

# convert into the correct timezone for your locale (mine is Arizona so we follow Mountain Standard)
attr(system_time,"tzone") <- "MST"

msg_body <- paste("08-combine-data-by-site.R", "completed at", system_time, sep = " ")

RPushbullet::pbPost(type = "note", title = "Script Completed", body = msg_body)
